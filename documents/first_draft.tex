\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{stmaryrd}
\usepackage{url}
\usepackage{hyperref}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}
\theoremstyle{plain}
\newtheorem{mydef}[thm]{Definition}
\theoremstyle{definition}
\newtheorem{myex}[thm]{Example}
\usepackage[english]{babel}
\newcommand\tab[1][1cm]{\hspace*{#1}}
\usepackage{qtree}
\usepackage{tabularx}

\author{First Draft}
\title{Learning of Safety Controllers using Horn Constraints}

\begin{document}
\maketitle
\begin{abstract}
Infinite-duration two-person games are a popular formalism for synthesizing reactive controllers. In this work, we consider safety games (which arise from safety specifications) and show how safety controllers can be computed using machine learning. At the heart of our approach lies a novel decision tree learning algorithm that learns from Horn constraints and has recently been developed for the verification of concurrent programs. We implement a teacher and show how we can use the Horn learner to synthesize a controller. Furthermore, we show that there is a bounded algorithm that guarantees the learning process to terminate in polynomial time.
\end{abstract}

\section{Introduction}

We are looking to synthesize reactive controllers subject to safety specifications. We model the interaction between system and environment as a two-player safety game over possibly infinite graphs. Thus, the usual approach of fixpoint computation doesn't work on this kind of problems. Instead, we use a a teacher learning setting to learn a controller. The teacher has knowledge about the game and the learner proposes hypothesis about a winning set in the game. 

Based on the ICE learning framework the teacher responds with counterexamples. Those counterexamples can have four different types: positive, negative, existential implication and universal implication counterexamples. We use a decision tree learner for Horn Samples and need to encoded our counterexamples accordingly to fit those requirements. The learning takes place iteratively. In each iteration the learner proposes a hypothesis to the teacher and the teacher responds with either a counterexample or none. The latter meaning that the hypothesis is indeed a winning set. Otherwise the learner incorporates the counterexample in the next iteration of synthesizing a hypothesis.  

One result of this paper is that we can transform a Game Sample that consists of such counterexamples into a Horn Sample that consists of Horn constraints. This allows us to use the already existing Horn learner\cite{d2017horn}. The teacher itself is based on the symbolic computation of counterexamples, used in "An Automaton Learning Approach to Solving Safety Games over Infinite Graphs" \cite{neider2016automaton}. But instead of using automata, we use SMT-solver to compute the counterexamples. This form of computation allows us to avoid exponential blow up by allowing us to not iterate over all possible solution but instead have an symbolic representation of our game.

In detail, we learn a winning set for the controlled system.  With the help of a winning set we can determine a strategy by staying inside the winning set. Once in the winning set the controlled system can force to stay inside this set of vertices. 

The main results of this paper are how to use a decision tree learner for Horn Samples to compute a winning set. A teacher using an SMT-solver to compute the counterexamples needed for the learner and the transformation of the Game Sample that consists of those counterexamples to a Horn Sample. Finally, we show that using our algorithm with the bounded Horn learner is guaranteed to converge.



\section{Overview}
We give an overview of all definitions that will be used during this paper. These include the definition of safety games and horn constraints. One of the first things we have to understand is the notion of data points.
 
We use data points over a domain $D$ to describe points in our graph. Our learner can use Horn constraints to build a tree $\mathcal{T}$, which evaluates data points to a boolean value ($\mathcal{T}:D \to \mathbb{B}$). The learning algorithm constructs a tree using a set of base predicates which evaluate to $true$ or $false$ on data points.
We can see, that a boolean function $f: D \to \mathbb{B}$ induces a valuation $v: D \to {true,false}$, where $x \mapsto_f 1 \iff x\mapsto_v true$.
\subsection{Safety Games}

First of we start with some definitions needed to understand safety games.


We say that a safety game is a infinite duration game over a graph with a countable set of vertices. Safety games are played on an arena $\mathfrak{A} = (V_{0}, V_{1}, E)$, where $V_{0}$ and $V_{1}$ are disjoint and countable sets of vertices and $E$ defines a directed edge relation $E \subseteq V \times V$. We write successor of a set of vertices as $E(X) = \{y \;|\; \exists x \in X : (x,y) \in E\}$.
The initial vertices of a safety game are defined in the following way. $\mathfrak{G} = (\mathfrak{A}, F, I)$, $\mathfrak{A} = (V_{0}, V_{1}, E)$ is an arena, $F \subseteq V$ a set of \textit{safe vertices} and $I \subseteq F$ a set of \textit{initial vertices}. Next, we explain how safety games are played. We have two players, Player 0 and Player 1 and each of them owns a set of vertices. Player 0 owns the vertices that are in $V_{0}$ and Player 1 the vertices in $V_{1}$. The safety game starts when a initial token is placed on one initial vertex $v_{0} \in I$. The player owning the vertex can move the token to one of its successors. Since we are talking about infinite duration games, the process is repeated ad infinitum. This yields a infinite sequence of vertices $v_{0}v_{1}...$ that we call a \textit{play} iff $v_{0} \in I$ and $(v_{i}, v_{i+1}) \in E$ for all $i \in \mathbb{N}$. A play is \textit{winning} for Player 0 if $v_{i} \in F$ for all $i \in \mathbb{N}$. Otherwise the play is \textit{winning} for Player 1.

A strategy for Player $\sigma$, $\sigma \in \{0,1\}$, is a mapping $f_{\sigma} : V*V_{\sigma} \to V$. A winning strategy yields a winning play for any play that is created using the strategy. We can get a winning strategy if you look at the \textit{winning set} of a safety game which is defined as follows.
\paragraph*{Winning set} For a safety game $\mathfrak{G} = (\mathfrak{A}, F, i)$ over the arena $\mathfrak{A} = (V_{0}, V_{1}, E)$, a winning set is a set $W \subseteq V$ satisfying
\begin{enumerate}
\item $I \subseteq W$
\item $W \subseteq F$
\item $E(\{v\}) \cap W \neq \emptyset$ for all $v \in W \cap V_{0}$
\item $E(\{v\}) \subseteq W$ for all $v \in W \cap V_{0}$
\end{enumerate}

A winning strategy for Player 0 would be to move inside the winning set. This can be proven with a induction over the length of plays.

\subsection{Teacher and learner interaction}

In this section we explain how the teacher and the learner interact with each other.
The goal is to compute winnings sets. Winnings sets have the above defined properties. We will follow the ICE-learning approach by learning with counterexamples, meaning if one of the properties is unsatisfied we can generate one counterexample. 

We can give positive counterexamples if property $I \subseteq W$ is unsatisfied and a negative counterexample if $W \subseteq F$ is unsatisfied.
For the third property we can give existential counterexamples ($ x \to (x_1 \vee  ... \vee x_k)$) and for the fourth we can give universal counterexamples ($ x\to (x_1 \wedge ... \wedge x_k)$). We will show later that it is possible to transform those counterexamples into horn constraints.

The teacher has knowledge about the safety game and can verify winning sets that the learner proposes. The teacher gives the learner counterexamples if the proposed winning set is wrong. 
The learner makes a conjecture about the winning set using the counterexamples given by the teacher. Then he gives the teacher his constructed winning set.
The interaction between teacher and learner takes place in rounds. In each round the learner makes a new conjecture and the teacher either accepts it or gives the learner a counterexample for his conjecture.

The counterexamples of the teacher are not horn clauses. Our goal is it to use a Horn learner, thus we present a way how to transform the counterexamples given by teacher into Horn clauses, usable by the Horn learner. 

First, we need to formalize definitions about the counterexamples. Starting with a \emph{Game Sample} that is produced by the teacher, the goal is to have a \emph{Horn Sample} which can be used by the learner. 


\begin{mydef}
A \emph{Game Sample} is a set $S = (X,C)$, where $X$ is a set of positive and negative data points and $C$ is a set of formulas over $X$. Those formulas have the following structure:
\begin{enumerate}
\item $true \to x$
\item $x \to false$
\item $x \to x_1 \vee  ... \vee x_k$
\item $x \to x_1 \wedge ... \wedge x_k$ 
\end{enumerate}
\end{mydef}

\begin{mydef}
A \emph{Horn Sample} is a set $S = (X,C)$, where $X$ is a set of boolean variables and $C$ is a set consisting of Horn constraints over $X$. Those formulas have the following structure:
\begin{enumerate}
\item $true \to x$
\item $x \to false$
\item $x_1 \wedge...\wedge x_k \to x $
\end{enumerate}
\end{mydef}

Furthermore, we need to define what it means for a sample to be valid. We want a sample to be valid if it doesn't contradict itself, so we take an intuitive definition for validity.  
\begin{mydef}
A Horn Sample (X,C) is \emph{valid} $\iff $ there is a valuation $v$ such that $v \vDash C$.
\end{mydef}
\begin{mydef}
A Game Sample (X,C) is \emph{valid} $\iff $ there is a valuation $v$ such that $v \vDash C$.
\end{mydef}

\subsection{Horn constraints and how to use them with data points}
In this section we explain how the Horn Sample and the Game Sample are connected. In the Game Sample the Horn constraints are over a set of data points. But in the Horn Sample we have Horn constraints over a set of boolean variables. 

A Horn clause (or a Horn constraint) over $X$ is a disjunction of literals over $X$ with at most one positive literal.
These are not necessarily boolean formulas since the formulas can go over data points. Let $S :=(X,C)$ be a Game sample, where $C = \{c_1,..,c_m\}$ is a set of formulas that are constraints, but instead of variables we have data points, and $D(C) := \{D(c)|c \in C\}$ is a set of data points that appear in the formulas. But we want to evaluate  constraints over boolean variables. To do this we need a Sample $S' = (X',C')$, but this time we have Horn constraints over boolean variables. To get this sample we need define a bijective function $v: D(C) \to X';d \mapsto x_d$ that maps our data points to variables that we use in our Horn constraints. Additionally, $C'$ has the same Horn constraints like $C$, but every data point is replaced with its variable. 

We can now define a valuation for $X'$, $v_\mathcal{T}: X' \to \mathbb{B};x_d \mapsto t(v^{-1}(x_{d}))$. Where $t$ is the evaluation of the data point of $x_d$ on the decision tree.

 We say a Horn sample $S = (X,C)$ over boolean variables is consistent with a tree $\mathcal{T}$ if and only if $v_{\mathcal{T}} \vDash \bigwedge_{c \in C} c$.

In summary it can be said, therefore, that we can get a valuation over boolean variables over a Game Sample, transforming it into a Sample with the same constraints over boolean variables. We will need this observation to show how we can transform such a Game Sample into a Horn Sample.

\newpage

\section{Transformation from Game Sample to Horn Sample} \label{transformation}
We will begin by applying our observation from the previous section and look at a  Sample which is based on a Game Sample, but uses boolean variables instead.

Let $C$ be a set of formulas over a set of variables $X$. These formulas are of the following forms:
\begin{enumerate}
\item $true \to x$
\item $x \to false$
\item $ x \to (x_1 \vee  ... \vee x_k)$
\item $ x\to (x_1 \wedge ... \wedge x_k)$
\end{enumerate}

Define for a valuation $v$ over $X$ a new valuation $v': X \to \mathbb{B}; x \to 1-v(x)$.
Define a set of horn constraints $C'$. Add a new formula to $C'$ for every formula in $C$:
\begin{enumerate}
\item $\forall (true \to x\in C)$ add $(x \to false)$ to $C'$
\item $\forall (x \to false\in C)$ add $(true \to x)$ to $C'$
\item $\forall (x \to (x_1 \vee  ... \vee x_k)\in C)$ add $ ((x_1 \wedge...\wedge x_k) \to x)$ to $C'$
\item $\forall (x \to (x_1 \wedge ... \wedge x_k)\in C)$ add a set of formulas $\{x_i \to x | i \in \{1,...,k\}\}$ to $C'$.
\end{enumerate}
We can  see that $C'$ is a set of Horn constraints.

\begin{lem} \label{lemma}
\textit{Let $v$ be a valuation for a set of formulas $C$ like in the construction above. Let $v'$ be the valuation and $C'$ a set of horn constraints we get by applying the construction. Then:}
\end{lem}
\[v \vDash C \iff v' \vDash C'\]
\begin{proof} $\\ $
Let $v$ be a valuation for $C$. If $v \vDash C$ then $v \vDash \bigwedge_{c \in C} c$ and $\forall c \in C : v \vDash c$. Therefore, it is enough to perform a case analysis for each formula $c \in C$. We do this by applying the described construction above to $v$ and $C$:
\begin{enumerate}
\item We apply the construction to formulas of the form $c:= true \to x$ and get $c':= x\to false$:
\begin{equation*}
\begin{split}
v & \vDash c \iff v \vDash x \iff v' \nvDash x \iff v' \vDash c'
\end{split}
\end{equation*}

\item We apply the construction to formulas of the form $c:= x \to false$ and get $c':= true \to x$:
\begin{equation*}
\begin{split}
v & \vDash c \iff v \nvDash x \iff v' \vDash x \iff v' \vDash c'
\end{split}
\end{equation*}
%(v  \vDash x) \wedge
\item We apply the construction to formulas of the form $c:=(x \to (x_1 \vee  ... \vee x_k))$ and get $c':= (x_1 \wedge...\wedge x_k) \to x)$:
\begin{equation*}
\begin{split}
v &\vDash c \iff (\exists i\in \{1..k\}: v \vDash x_i) \vee (v \nvDash x) \\
\iff (v' &\vDash x) \vee ((\exists i\in \{1..k\}: v' \nvDash x_i)) \iff v' \vDash c'
\end{split}
\end{equation*}
\item We apply the construction to formulas of the form $c  := x \to (x_1 \wedge ... \wedge x_k)$ and get $c':=\{x_i \to x | i \in \{1,...,k\}\}$:
\begin{equation*}
\begin{split}
v  \vDash c \iff (\forall i\in \{1..k\}: &v \vDash x_i) \vee (v \nvDash x)\\
\iff (v' \vDash x) \vee ((\forall &i \in  \{1..k\}: v' \nvDash x_i)) \\
\iff (\forall i\in \{1..k\}:& v' \vDash  x_i \to x) \iff v' \vDash c'
\end{split}
\end{equation*}
\end{enumerate}
Summarizing the case analysis we can see that if we take any formula in $c \in C$ and transform it into a formula  $c' \in C'$ we get that $v \vDash c \iff v' \vDash c'$ and therefore $v \vDash C \iff v' \vDash C'$ for any valuation $v$.

\end{proof}
\begin{cor} \label{Korollar}

A formula $\psi$ is consistent with a Game Sample if and only if it is consistent with the corresponding Horn Sample.
\end{cor}
\begin{proof}
Let $\psi $ be a formula that is consistent with the Game Sample $S_{Game}$. Now we have to perform a case analysis again for each formula $c \in C_{Game}$.
\begin{enumerate}
\item
\begin{equation*}
\begin{split}
x \to x_1 \vee &\dots \vee x_n \; is \; consistent \; with \; \psi \\
\iff &(\exists i \in \{1,\dots,n\}: x_i \vDash \psi) \vee (x \nvDash \psi) \\
\iff &(x' \vDash \psi) \vee ((\exists i\in \{1..n\}: x_i' \nvDash \psi)) \\
\iff &x_1' \wedge \dots \wedge x_n' \to x' \; is \; consistent \; with \; \psi 
\end{split}
\end{equation*}
\item The rest of the cases are similarly proven.
\end{enumerate}
\end{proof}

\section{Bounded Decision Tree Learner for Horn Samples}

In this section, we show that we can build a decision tree learner that is guaranteed to terminate. We already know that the Horn Learner terminates on a sample\cite{d2017horn}, but what is left to show is that the teacher learner interaction eventually terminates. This is not obvious since the learner is learning over a set of formulas that involves numerical attributes, making it infinitely large.

We want to bound the maximum threshold that can occur in the inequalities the learner uses in the hypothesis. Thus, the amount of non equivalent boolean formulas that respect the restriction is finite. We iteratively bound the maximum threshold that occur in the inequalities and only increase the maximum threshold if there is no tree that respects the restriction and is consistent with the sample. The learner never proposes the same tree twice since the teacher will respond with an appropriate counterexample if the tree  is not a winning set. Furthermore, we won't increase the maximum threshold if there is a tree that respects the threshold and is a winning set. Thus, if there is a tree that is a winning set within a maximum threshold, the learner will find this tree in an finite amount of time.

To proof our claim, we want to use theorem \ref{Theorem1}. For this, we will define when a set of points is separable.
\begin{thm} \label{Theorem1}
If the input set of points X is separable and the input Horn Constraints are satisfiable, then the Learner always terminates with a decision tree consistent with the Horn sample (X,C).\cite{d2017horn}
\end{thm}

\begin{mydef}
Given an Horn sample $S = (X,C)$. Define the equivalence class $\equiv_m$ m on X: $\\ \\ $ 

$s \equiv_m s'$ $\iff$ there is no predicate of the form $a_i \leq$ $c$ with $|c| \leq$ $m \\ \tab \tab \tab \tab $ that separates $s$ and $s'$.
\end{mydef}

\begin{mydef}
We can augment a sample $S = (X,C)$ to obtain a \emph{m-augmented Horn Sample of S}, denoted by $S \oplus m$. To obtain $S \oplus m$, we first construct the set $E = \{(s,s')\;|\; s \equiv_m s'\}$ and add it to a our Horn Constraints, $(X,C \cup E)$.
\end{mydef}

Next, we want to show that if we have a Horn Sample that respect a maximum threshold m, our Horn Learner returns with a tree that is consistent with the sample. Our goal is to show that if we have a Game Sample, then our Horn Learner returns with a tree that is consistent with the Game Sample. But first we have to take detour to Horn Samples. We will use later our transformation to show that the same can be done for the Game Sample.

\begin{thm} \label{Horn Theorem}
Let $S$ be an Horn Sample. Then the following holds:
\begin{enumerate}
\item If $S \oplus m$ is not valid, then there is \emph{no Boolean formula with absolute maximum threshold m} that is consistent with $S$. In the overall learning loop, we would increment $m$ in this case and restart learning from $S$.

\item If $S \oplus m$ is valid, then calling our Decision Tree Learner for Horn Samples on $S \oplus m$ while restricting it to predicates that use thresholds with absolute values at most $m$ is \emph{guaranteed to terminate and return a tree that is consistent with S}. 
\end{enumerate}
\end{thm}
% anstatt s_i in S^+ usw. Learner findet kein assignment sodass, die horn klauseln erf√ºllbar sind? neeee
\begin{proof} $\\ $ 
\begin{enumerate}
\item Assume $S \oplus m$ is not valid. This means the Horn Solver returns with an unsat. Now for the sake of contradiction assume that there is a Boolean formula $f$ with absolute maximum threshold m that is consistent with S. Thus, $f$ satisfies all the new horn constraints added and consequently $S \oplus m$. Thus calling our Horn Solver on $S \oplus m$ should have returned with a boolean function $f'$ which assigns the values in $X$ according to f which is a contradiction to our Horn Solver returning unsat.

\item Assume now that $S \oplus m$ is valid.  We need to show that the input set of points is separable. For this assume that our bounded Decision Tree Learner for Horn Samples processes a node with the sample of $S \oplus m$. Moreover, assume that we can't find a split with threshold $|c| \leq m$. We know that the data points we want to split can't be pure. Otherwise, we have a split which assigns them all to one side. Thus, there are three different cases that can happen, two of those are analogous.
\begin{enumerate}
\item We have at least one positively classified data point p and at least one negatively classified data point n. Since we can't separate those two points, we know that they are in the same m-equivalence class. Thus, $(p,n)$ is a horn constraint in $S \oplus m$. This means our Horn Solver should have returned with unsat, since $(p,n)$ is an unsatisfiable horn constraint.

\item We have no negatively classified data points. Thus, we only have positively classified data points and unclassified data points. Since those points are inseparable, they are all in the same m-equivalence class. This means that they all have to have the same classification which is positive, making a split possible. This is a contradiction to our assumption that we can't separate those data points.

\item We have no positively classified data points. This is analogous to case (b).
\end{enumerate}

Furthermore, if $S \oplus m$ is valid, then there is a valuation that satisfies $C$.

Now, we have that the input set of points is separable and that $S \oplus m$ is valid and can use Theorem \ref{Theorem1} to get that the Learner always terminates with a decision tree consistent with the Horn sample $(X,C)$.
\end{enumerate}
\end{proof}

Now, that we have shown that our Horn learner works with a Horn Sample, we show that it also works if we start with a Game Sample.
\begin{thm}
Let $S_{Game}$ be a Game Sample. Then the following holds:
\begin{enumerate}
\item If $S \oplus m$ is not valid, then there is \emph{no Boolean formula with absolute maximum threshold m} that is consistent with $S$. In the overall learning loop, we would increment $m$ in this case and restart learning from $S$.

\item If $S \oplus m$ is valid, then calling our Decision Tree Learner for Horn Samples on $S \oplus m$ while restricting it to predicates that use thresholds with absolute values at most $m$ is \emph{guaranteed to terminate and return a tree that is consistent with S}. 
\end{enumerate}
\end{thm}
\begin{proof} $\\ $
\begin{enumerate}
\item $S_{Game} \oplus m$ not valid $\Rightarrow$ there is no valuation $v$ such that $v \vDash C$ $\overset{Lemma \;\ref{lemma}} \Rightarrow $ there is no valuation $v'$ for the Horn Sample $S_{Horn} \oplus m$. $\Rightarrow$ $S_{Horn} \oplus m$ is not valid $\overset{Theorem \;\ref{Horn Theorem}}\Rightarrow$ there is no Boolean formula with maximum threshold m that is consistent with $S_{Horn}$

Assume there exists a Boolean formula $f'$ with maximum threshold m such that $f'$ is consistent with $S_{Game}$. With Corollary \ref{Korollar} we know that $f'$ is also consistent with $S_{Horn}$. Thus, we found a Boolean formula with maximum threshold m that is consistent with $S_{Horn}$ which is a contradiction to $S_{Game} \oplus m$ being not valid.

\item $S_{Game} \oplus m$ is valid $\overset{Lemma \;\ref{lemma}}\Rightarrow$ $S_{Horn} \oplus m$ is valid $\overset{Theorem \; \ref{Horn Theorem}} \Rightarrow$ our Learner for Horn Samples returns with a tree that is consistent with $S_{Horn}$, this tree encodes a Boolean formula that is consistent with $S_{Horn}$ $\overset{Corollary\; \ref{Korollar}}\Rightarrow$ this tree is also consistent with $S_{Game}$
\end{enumerate}
\end{proof}
\newpage
\section{Experiments}
In order to compare how effective the Horn learner is, we implemented a teacher using Microsofts Z3 \cite{de2008z3} constraint solver and used the transformation mentioned in section \ref{transformation} such that the Horn learner can learn using Horn constraints. The samples our teacher produces are Game samples. Furthermore, we will use the proposed Horn learner in "Horn-ICE Learning for Synthesizing Invariants and Contracts" \cite{d2017horn}. We will compare our results with the Sat learner and the RPNI learner mentioned in "An Automaton Learning Approach to Solving Safety Games over Infinite Graphs" \cite{neider2016automaton}. 

Again we will restrict our experiment in a way that each vertex has a finite number of outgoing edges. In our case this number also needs to be bounded. We need the limitation since our teacher is required to return a counterexample which lists all outgoing edges if the counterexample is an existential or universal implication.

We conducted the experiments on a intel core i3-4005U with Ubuntu 18.04 as operating system. The memory limit is at 3.8 GiB.

We considered following examples, for an detailed explanation we refer to the appendix:

\begin{enumerate}
\item \emph{Diagonal game:} In this game we have an infinite, discrete two-dimensional grid world and a robot. Both players can move the robot in an vertical, horizontal or diagonal direction by one cell. Player 0 wins if the robot stays within two cell around the diagonal.
\item \emph{Limited Diagonal game:} A variation of the diagonal game, used in "An Automaton Learning Approach to Solving Safety Games over Infinite Graphs" \cite{neider2016automaton}. Player 0 can only control the robot's vertical movement and Player 1 the horizontal.
\item \emph{Box game:} A variation of the diagonal game. Player 0 wins if the robot stays within a horizontal stripe of width tree.
\item \emph{Limited Box game:} A variation of the box game, used in "An Automaton Learning Approach to Solving Safety Games over Infinite Graphs" \cite{neider2016automaton}. Player 0 can only control the robot's vertical movement and Player 1 the horizontal.
\item \emph{Solitary box game:} Box game with only Player 0 controlling the robot.
\item \emph{Evasion game:} Two robots are moving in an infinite, discrete two-dimensional grid world. The robots take turns moving at most one cell in any direction. Both players control one robot. Player 0's objective is to avoid getting caught by Player 1's robot.
\item \emph{Follow game:} A version of the evasion game where Player 0's objective is to keep the robot within a distance of two cells. (Manhattan distance)
\item \emph{Program repair game:} The program-repair game described by Beyene et al. \cite{beyene2014constraint}
\item \emph{Track game:} One Robot is moving on a infinte track. Both players control the robot and can decide to move the robot left, right or stay. The objective of Player 0 is to stay in a safe area.
\item \emph{Square game:} A variation of the box game, where the space Player 0 is allowed to move is limited to a square.
\item \emph{Multi Track game:} A variation of the track game with multiple tracks and one robot per track.
\end{enumerate}

\begin{table}[h]
 \caption{Results of the Experiments}
\begin{tabularx}{\textwidth}{p{0.1\textwidth}p{0.1\textwidth} p{0.1\textwidth} p{0.1\textwidth}p{0.1\textwidth} p{0.1\textwidth} p{0.1\textwidth} p{0.08\textwidth}}
\hline
& \multicolumn{7}{c}{Horn Learner}\\ \hline
Game & Time in s & Iter.& Size & $|Pos|$ & $|Neg|$ & $|Ex|$ & $|Uni|$ \\ \hline
Diagonal & 4.435 & 25 & 6 & 11 & 7 & 4 & 2 \\ 
Diagonal Limited & 2.929 & 24 & 6 & 11 & 6 & 4 & 2 \\ 
Box & 0.641 & 9 & 4 & 2 & 3 & 0 & 3 \\
Box Limited & 0.202 & 4 & 3 & 1 & 2 & 0 & 0 \\
Solitary Box & 0.697 & 4 & 3 & 1 & 2 & 0 & 0 \\
Evasion & 5.206 & 16 & 4 & 1 & 4 & 7 & 3 \\ 
Follow & 44.097 & 181 & 14 & 1 & 129 & 32 & 18 \\ 
Program-repair & 1.129 & 14 & 11 & 1 & 2 & 10 & 0 \\
Laufband & 0.982 & 5 & 3 & 1 & 1 & 1 & 1 \\ 
Quadrat 3x3 & 6.213 & 61 & 13 & 1 & 18 & 19 & 22 \\
Quadrat 5x5 & 4.195 & 41 & 16 & 1 & 13 & 16 & 10 \\
Multitank 3 & 10.722 & 115& 12 & 1 & 82 & 10 & 21 \\

\end{tabularx}
 \label{tab:TESTS} 
\end{table}

\newpage
\section{Conclusion}

\newpage
\bibliographystyle{plain}
\bibliography{lib}
\newpage
\appendix
\section{Appendix}
\subsection{Game Examples}
\begin{figure}[h] \label{Laufband}
  \caption{In this example we have a robot moving on an infinite, discrete one-dimensional grid. Two players can control the robot, Player 0 and Player 1. The goal of Player 0 is to let the robot stay in the safe zone, the goal of Player 1 is to move the robot out of it. Both player take turns moving the robot, moving it at most one field in one direction.
.}
  \centering
    {\includegraphics[width=8.0cm]{laufband.png}} 

\end{figure}


\begin{figure}[h]\label{Wassertank}
  \caption{Here we have a water tank and two players. This example works similiar to figure above, but this time the safe zone is limited by two sides since a water tank has only a finite capacity and we don't want the tank to run out of water.
}
  \centering
    {\includegraphics[width=8.0cm]{wasser_tank.png}} 
\end{figure}

\begin{figure}
  \caption{Consider a robot on an infinite two-dimensional grid with two players playing against each other. Both player can move the robot in an arbitrary direction by at most one field. Player 0 tries to keep the robot in a safe square, while Player 1 tries to move the robot out of the square.
}
  \centering
    {\includegraphics[width=8.0cm]{quadrat.png}} 
\end{figure}


\begin{figure}
  \caption{This example works like the water tank example, played with multiple tanks. Each player can control the tank in its turn and can decide to either fill the tank or empty the tank by one unit.}
  \centering
    {\includegraphics[width=8.0cm]{multi_wasser_tank.png}} 
\end{figure}






\begin{figure}
  \caption{In this example we have an infinite, discrete two-dimensional grid that is limited by two straight lines. Both player can move the robot in an arbitrary direction. Player 0 wins if the robot stays within the area limited by the straight lines. Player 1 wins if the robot moves out of it.}
  \centering
    {\includegraphics[width=8.0cm]{catch.png}} 
\end{figure}

\begin{figure}
  \caption{We have two robots moving in an infinite, discrete two-dimensional grid. Player 0's objective is to avoid being caught by Player 1.
}
  \centering
    {\includegraphics[width=8.0cm]{zwei_geraden.png}} 
\end{figure}

\end{document}